{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14372d2-abe3-4322-91cd-58591fd241f6",
   "metadata": {},
   "source": [
    "## Working with Dicom Files\n",
    "\n",
    "The point of this notebook is to:\n",
    "* explore how the information in a dicom file is organized and stored\n",
    "* what to remove from the dicom header and how to remove it\n",
    "    * how do we access the US region coordinates using Pydicom\n",
    "* what to remove from the pixel data and how to remove it\n",
    "\n",
    "The article <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3354356/\">Managing DICOM images: Tips and tricks for the radiologist</a> \n",
    "contains an easily readable overview of the dicom format and says this about deidentification:\n",
    "\n",
    "The common tags that indicate the patient identity include the patient's name, age, sex, birth date, hospital identity number, ethnic group, occupation, referring physician, institution name, study date, and DICOM Unique Identifiers (UIDs). As described earlier, such demographic information of the patient and a host of other information about the imaging study is encoded within an image header. The data may or may not be displayed on the screen, but the information can be extracted from the header by anyone who has access to the DICOM file. Several educational resources using DICOM files are available for radiology students on the World Wide Web. Creating and accessing such electronic teaching files often involve transmission of DICOM data over the Internet. In the interest of patient confidentiality, all information identifying the patient should be removed from the DICOM header when a DICOM file is uploaded for such purposes.\n",
    "\n",
    "Respecting the patient's privacy is important when images are used in presentations, teaching files, or publications. A simple and easy method of ensuring this is by converting and exporting the DICOM file into other image formats such as JPEG or TIFF. The header information is lost and patient identity cannot be obtained from the resultant image. Another method is “anonymization,” whereby all patient information is removed from the DICOM header.[3] This is achieved by using software like DicomWorks, ImageJ, and FP Image.[7,8,10] Specifically, all tags contained in groups “0008” (study information) and “0010” (patient information) of the DICOM header should be removed and replaced during anonymization.\n",
    "\n",
    "## The early pipeline:\n",
    "\n",
    "First, break the large Datamart file into smaller Datamart files with at most 100 cases each.  (csv_splitter function)\n",
    "\n",
    "For each smaller Datamart file that does not begin with PROC\n",
    "\n",
    "* Build a matching Notion query file\n",
    "* Use Notion to retrieve zip archive of dicom files and anonymization map\n",
    "* Store small datamart file, zip archive, and anon_map file using 00007_datamart.csv, 00007_dicoms.zip, 00007_anon_map.csv\n",
    "* Append 00007_anon_map.csv to master_anon_map.csv\n",
    "* Rename 00007_anon_map.csv to PROC_00007_anon_map.csv\n",
    "* Extract the dicom files from 00007_dicoms.zip and deidentify them.  Save them in data_anon/dicoms/00007_dicoms\n",
    "* Clean and anonymize the 00007_datamart.csv (add anonymized ids, delete any columns with PHI, rename and reorder columns).  Save in data_anon/dicoms/00007_dicoms\n",
    "* Compress and archive data_anon/dicoms/00007_dicoms to 00007_dicoms.zip.  Delete folder.\n",
    "* Append 00007_datamart.csv to master_datamart.csv (will have PHI)\n",
    "* Rename 00007_datamart.csv to PROC_datamart_00007.csv\n",
    "\n",
    "On the Mayo side we will store:\n",
    "\n",
    "* All small datamart and anon_map files.\n",
    "* The Notion zip archives.\n",
    "* The master_datamart and master_anon_map files.\n",
    "* No need to store Notion queries, extracted images, etc.\n",
    "* We can always rebuild de-identified dicoms from the stored zip archives and other saved data.\n",
    "\n",
    "On the UWL side we will store \n",
    "\n",
    "* de-identified dicoms (at least temporarily)\n",
    "* anonymized small datamart files (at least temporarily)\n",
    "* Tristan will use these to build the database\n",
    "\n",
    "## Things to update, fix, refactor\n",
    "\n",
    "* to make the anonymization safer we should delete everything in the dicom file that we don't intend to keep\n",
    "    * in the opposite direction we may need to put back the time for each image so we can fill in missing lateralities\n",
    "* rewrite create_dcm_filename to use dicom_media_type\n",
    "\n",
    "## New functionalities to add\n",
    "\n",
    "* build an anonymization map for biopsy accession numbers\n",
    "    * incrementally ingest datamart files and add to the biop anon csv\n",
    "* write a general function that inputs a datamart type file and selects two columns, output is a notion query file (or multiple)\n",
    "* include the csv splitter function in this notebook\n",
    "* write a function that inputs datamart type files along with both anonymization maps and outputs a growing anonymized datamart file\n",
    "* write a function that gets notion query reports and adds the descriptions to the master anonymized datamart file\n",
    "* write a function that takes biopsy query reports, extracts and adds the laterality to any biopsy anon map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3249b0-9e4d-4f61-aa46-d31f9ab692d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pydicom import dcmread\n",
    "from hashlib import sha1\n",
    "import pydicom\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9159984f-2bf1-4ab2-9286-30b6f2f104b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anon_callback(ds, element):\n",
    "    \"\"\"used with \"walk\" to loop over dicom and anonymize entries\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    names = ['SOP Instance UID','Study Time','Series Time','Content Time',\n",
    "             'Study Instance UID','Series Instance UID','Private Creator',\n",
    "             'Media Storage SOP Instance UID',\n",
    "             'Implementation Class UID']\n",
    "    \n",
    "    if element.name in names:\n",
    "        element.value = \"anon\"\n",
    "\n",
    "    if element.VR == \"DA\":\n",
    "        date = element.value\n",
    "        date = date[0:4] + \"0101\" # set all dates to YYYY0101\n",
    "        element.value = date\n",
    "\n",
    "    if element.VR == \"TM\":\n",
    "        element.value = \"anon\"\n",
    "\n",
    "def deidentify_dicom_dataset( ds ):\n",
    "    \"\"\"remove patient information from pydicom dataset that Notion has partially deanonymized\n",
    "\n",
    "    Args:\n",
    "        ds: pydicom dataset from reading dicom file with pydicom\n",
    "\n",
    "    Returns:\n",
    "        out_ds: pydicom dataset with private information removed from header and image\n",
    "        is_video:  True for Multi-frame dicom, False for single image\n",
    "        hash: sha1 hash of str(ds.pixel_array) for 'unique id' and \n",
    "    \"\"\"\n",
    "\n",
    "    ds.remove_private_tags() # take out private tags added by notion or otherwise\n",
    "\n",
    "    ds.file_meta.walk(anon_callback)\n",
    "    ds.walk(anon_callback)\n",
    "\n",
    "    media_type = ds.file_meta[0x00020002]\n",
    "    is_video = str(media_type).find('Multi-frame')>-1\n",
    "    is_secondary = str(media_type).find('Secondary')>-1\n",
    "    if is_secondary:\n",
    "        y0 = 101\n",
    "    else:\n",
    "        if (0x0018, 0x6011) in ds:\n",
    "            y0 = ds['SequenceOfUltrasoundRegions'][0]['RegionLocationMinY0'].value\n",
    "        else:\n",
    "            y0 = 101\n",
    "\n",
    "    if 'OriginalAttributesSequence' in ds:\n",
    "        del ds.OriginalAttributesSequence\n",
    "\n",
    "    # crop patient info above US region \n",
    "\n",
    "    arr = ds.pixel_array\n",
    "    \n",
    "    if is_video:\n",
    "        arr[:,:y0] = 0\n",
    "    else:\n",
    "        arr[:y0] = 0\n",
    "\n",
    "    ds.PixelData = arr.tobytes()\n",
    "\n",
    "    return ds\n",
    "\n",
    "def create_dcm_filename( ds ):\n",
    "    \"\"\"uses info from dicom file to create informative filename\n",
    "\n",
    "    Args:\n",
    "        ds:  dataset extracted from dicom file\n",
    "\n",
    "    Returns:\n",
    "        filename:  \"patient id\"_\"acc num\"_\"type\"_\"hash\".dcm\n",
    "             \"patient id\" is already anonymized by Notion\n",
    "             \"acc num\" is already anonymized by Notion\n",
    "             \"type\" is image, video (multi-frame array of images), or second (weird type of image, rare)\n",
    "             \"hash\" is sha1 hash created from ds.pixel_array (could use to check for duplicates)\n",
    "    \"\"\"\n",
    "    patient_id = ds.PatientID.rjust(8,'0')\n",
    "    accession_number = ds.AccessionNumber.rjust(8,'0')\n",
    "\n",
    "    media_type = ds.file_meta[0x00020002]\n",
    "    is_video = str(media_type).find('Multi-frame')>-1\n",
    "    is_secondary = str(media_type).find('Secondary')>-1\n",
    "    \n",
    "    if is_video:\n",
    "        media = 'video'\n",
    "    elif is_secondary:\n",
    "        media = 'second'\n",
    "    else:\n",
    "        media = 'image'\n",
    "\n",
    "    image_hash = sha1( ds.pixel_array ).hexdigest()\n",
    "\n",
    "    filename = f'{media}_{patient_id}_{accession_number}_{image_hash}.dcm'\n",
    "\n",
    "    return filename\n",
    "\n",
    "def dicom_media_type( dataset ):\n",
    "    type = str( dataset.file_meta[0x00020002].value )\n",
    "    if type == '1.2.840.10008.5.1.4.1.1.6.1': # single ultrasound image\n",
    "        return 'image'\n",
    "    elif type == '1.2.840.10008.5.1.4.1.1.3.1': # multi-frame ultrasound image\n",
    "        return 'multi'\n",
    "    else:\n",
    "        return 'other' # something else\n",
    "\n",
    "def extract_deidentify_dcm_files(directory, target_directory):\n",
    "    \n",
    "    # Create the target directory if it doesn't exist\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "    \n",
    "    # Get a list of all ZIP files in the directory\n",
    "    zip_files = [filename for filename in os.listdir(directory) if filename.endswith('.zip') and not filename.startswith('PROC_')]\n",
    "    \n",
    "    # Loop over each ZIP file\n",
    "    for zip_file in zip_files:\n",
    "        # create target subdirectory\n",
    "        zip_name, extension = os.path.splitext(zip_file)\n",
    "        target_subdirectory = target_directory +  zip_name + '_anon/'\n",
    "        os.makedirs(target_subdirectory, exist_ok = True)\n",
    "        \n",
    "        # Open the ZIP file\n",
    "        zip_path = os.path.join(directory, zip_file)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Loop over each file in the ZIP file\n",
    "            for member in zip_ref.namelist():\n",
    "                if member.endswith('.dcm'):\n",
    "\n",
    "                    # Read the DICOM file using PyDICOM\n",
    "                    with zip_ref.open(member, 'r') as dicom_file:\n",
    "                        dataset = pydicom.dcmread(dicom_file)\n",
    "\n",
    "                        # check to make sure dicom has image or multi-frame video, else ignore\n",
    "                        # if its an image makde sure SequenceOfUltrasoundRegions is present\n",
    "                        media_type = dicom_media_type( dataset )\n",
    "                        if (media_type == 'image' and (0x0018, 0x6011) in dataset) or media_type=='multi':\n",
    "    \n",
    "                            # remove patient information from the dicom dataset\n",
    "                            dataset = deidentify_dicom_dataset(dataset)\n",
    "    \n",
    "                            # create new filename from header and hashed image\n",
    "                            new_filename = create_dcm_filename( dataset ) \n",
    "                            \n",
    "                            # Set the target path to write the DICOM file\n",
    "                            target_path = os.path.join(target_subdirectory, new_filename)\n",
    "                        \n",
    "                            # Write the DICOM dataset to a new DICOM file\n",
    "                            dataset.save_as(target_path)\n",
    "        \n",
    "        # Rename the processed file with 'PROC_' at the beginning\n",
    "        processed_file = os.path.join(directory, 'PROC_' + zip_file)\n",
    "        os.rename(zip_path, processed_file)\n",
    "\n",
    "def check_uncompressed( ds ):\n",
    "    type = ds.file_meta.TransferSyntaxUID\n",
    "    uncompressed_types = ['1.2.840.10008.1.2.1','1.2.840.10008.1.2.2','1.2.840.10008.1.2']\n",
    "    return type in uncompressed_types\n",
    "\n",
    "def extract_deidentify_dcm_file(zip_path, zip_file, target_directory):\n",
    "    \n",
    "    os.makedirs(target_directory, exist_ok = True)\n",
    "    \n",
    "    # Open the ZIP file\n",
    "    full_path_zip_file = os.path.join(zip_path, zip_file)\n",
    "    with zipfile.ZipFile(full_path_zip_file, 'r') as zip_ref:\n",
    "        # Loop over each file in the ZIP file\n",
    "        for member in zip_ref.namelist():\n",
    "            if member.endswith('.dcm'):\n",
    "\n",
    "                # Read the DICOM file using PyDICOM\n",
    "                with zip_ref.open(member, 'r') as dicom_file:\n",
    "                    #print(dicom_file)\n",
    "                    dataset = pydicom.dcmread(dicom_file)\n",
    "\n",
    "                    # check to make sure dicom has image or multi-frame video, else ignore\n",
    "                    # if its an image makde sure SequenceOfUltrasoundRegions is present\n",
    "                    media_type = dicom_media_type( dataset )\n",
    "\n",
    "#                    is_image = media_type == 'image'\n",
    "                    is_secondary = str(media_type).find('Secondary')>-1\n",
    "                    if is_secondary:\n",
    "                        print('SECONDARY:',dicom_file)\n",
    "                    \n",
    "                    if (( media_type == 'image' and (0x0018, 0x6011) in dataset) or media_type=='multi'):\n",
    "\n",
    "                        # if image is compressed, decompress it and change colorspace if needed\n",
    "                        is_compressed = not check_uncompressed(dataset)\n",
    "                        if is_compressed:\n",
    "                            dataset.decompress()\n",
    "                            arr = dataset.pixel_array\n",
    "                            color_space_in = dataset.PhotometricInterpretation\n",
    "                            if color_space_in not in ['MONOCHROME2','RGB']:\n",
    "                                color_space_out = 'RGB'\n",
    "                                arr = pydicom.pixel_data_handlers.util.convert_color_space(arr, \n",
    "                                                                                           color_space_in, \n",
    "                                                                                           color_space_out, \n",
    "                                                                                           True)\n",
    "                                dataset.PixelData = arr.tobytes()\n",
    "                                dataset.PhotometricInterpretation = color_space_out\n",
    "                                \n",
    "\n",
    "                        # remove patient information from the dicom dataset\n",
    "                        dataset = deidentify_dicom_dataset(dataset)\n",
    "                        \n",
    "                        # create new filename from header and hashed image\n",
    "                        new_filename = create_dcm_filename( dataset ) \n",
    "                        \n",
    "                        # Set the target path to write the DICOM file\n",
    "                        target_path = os.path.join(target_directory, new_filename)\n",
    "                    \n",
    "                        # Write the DICOM dataset to a new DICOM file\n",
    "                        #print(dicom_file)\n",
    "                        dataset.save_as(target_path)\n",
    "    \n",
    "    # Rename the processed file with 'PROC_' at the beginning\n",
    "    new_zip_file = os.path.join(zip_path,f'PROC_{zip_file}')\n",
    "    os.rename(full_path_zip_file, new_zip_file)\n",
    "\n",
    "def append_to_csv(target_file, input_file):\n",
    "    # Add prefix \"PROC_\" to the input filename\n",
    "    input_dir = os.path.dirname(input_file)\n",
    "    input_filename = os.path.basename(input_file)\n",
    "    input_filename_with_prefix = \"PROC_\" + input_filename\n",
    "    input_file_with_prefix = os.path.join(input_dir, input_filename_with_prefix)\n",
    "\n",
    "    # Check if the target file exists\n",
    "    target_exists = os.path.exists(target_file)\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(input_file, 'r', newline='') as input_csv_file:\n",
    "        input_csv_reader = csv.reader(input_csv_file)\n",
    "        input_rows = list(input_csv_reader)\n",
    "\n",
    "    # Check if the input file has any rows\n",
    "    if len(input_rows) == 0:\n",
    "        print(\"Input file is empty. No rows to append.\")\n",
    "        return\n",
    "\n",
    "    # Determine if the target file already has a header\n",
    "    target_has_header = False\n",
    "    if target_exists:\n",
    "        with open(target_file, 'r', newline='') as target_csv_file:\n",
    "            target_csv_reader = csv.reader(target_csv_file)\n",
    "            target_has_header = next(target_csv_reader, None) is not None\n",
    "\n",
    "    # Open the target file for appending\n",
    "    with open(target_file, 'a', newline='') as target_csv_file:\n",
    "        target_csv_writer = csv.writer(target_csv_file)\n",
    "\n",
    "        # If target file doesn't have a header, write the header from the input file\n",
    "        if not target_has_header:\n",
    "            target_csv_writer.writerow(input_rows[0])\n",
    "\n",
    "        # Write the rows from the input file, skipping duplicate rows and the header if it exists\n",
    "        if target_has_header:\n",
    "            input_rows = input_rows[1:]\n",
    "\n",
    "        # Get the existing rows in the target file\n",
    "        existing_rows = []\n",
    "        if target_exists:\n",
    "            with open(target_file, 'r', newline='') as target_csv_file:\n",
    "                existing_csv_reader = csv.reader(target_csv_file)\n",
    "                existing_rows = list(existing_csv_reader)\n",
    "\n",
    "        # Append only the non-duplicate rows\n",
    "        for row in input_rows:\n",
    "            if row not in existing_rows and row != input_rows[0]:\n",
    "                target_csv_writer.writerow(row)\n",
    "                existing_rows.append(row)\n",
    "\n",
    "    # Rename the input file with the \"PROC_\" prefix\n",
    "    os.rename(input_file, input_file_with_prefix)\n",
    "\n",
    "\n",
    "def remove_proc_prefix(folder):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder)\n",
    "    \n",
    "    # Loop over each file\n",
    "    for filename in files:\n",
    "        old_path = os.path.join(folder, filename)\n",
    "        \n",
    "        # Check if the file starts with 'PROC_'\n",
    "        if filename.startswith('PROC_'):\n",
    "            new_filename = filename[5:]  # Remove the 'PROC_' prefix\n",
    "            new_path = os.path.join(folder, new_filename)\n",
    "            \n",
    "            # Rename the file by removing the 'PROC_' prefix\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "def remove_spaces_from_column_names(dataframe):\n",
    "    dataframe.columns = dataframe.columns.str.replace(' ', '')\n",
    "    return dataframe\n",
    "\n",
    "def merge_anon_into_datamart(datamart_csv_file, anon_map_csv_file, target_directory):\n",
    "\n",
    "    \"\"\"use anonymization map to replace accession and patient numbers in datamart file, remove patient info, and rename datamart features\n",
    "\n",
    "    Args:\n",
    "        datamart_csv_file: full path to unanonymized datamart csv file\n",
    "        anon_map_csv_file: full path to anonymization map file\n",
    "        target_directory:  path where anonymized datamart file should be saved (filename derived from input datamart file or save path)\n",
    "\n",
    "    Returns:\n",
    "        nothing - output is csv file on disk\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    datamart_df = remove_spaces_from_column_names( pd.read_csv(datamart_csv_file) )\n",
    "\n",
    "    print(anon_map_csv_file)\n",
    "    anon_map_df = remove_spaces_from_column_names( pd.read_csv(anon_map_csv_file) )\n",
    "    \n",
    "    datamart_joined_df = pd.merge(datamart_df, anon_map_df,left_on='ACCESSIONNUMBER',right_on='OriginalAccessionNumber',how='left')\n",
    "\n",
    "    col_to_keep = ['AnonymizedPatientID',\n",
    "                   'AnonymizedAccessionNumber',\n",
    "                   'SCORE_CD',\n",
    "                   'BIOP_SCORE',\n",
    "                   'SEQ',\n",
    "                   'A1_PATHOLOGY_TXT',\n",
    "                   'DENSITY_TXT',\n",
    "                   'AGE',\n",
    "                   'RACE',\n",
    "                   'ETHNICITY']\n",
    "    \n",
    "    new_names = {\"AGE\":\"Age\",\n",
    "                 \"RACE\":\"Race\",\n",
    "                 \"ETHNICITY\":\"Ethnicity\",\n",
    "                 \"DENSITY_TXT\":\"Density_Desc\",\n",
    "                 \"A1_PATHOLOGY_TXT\":\"Path_Desc\",\n",
    "                 \"SCORE_CD\":\"BI-RADS\",\n",
    "                 \"SEQ\":\"Biop_Seq\",\n",
    "                 \"BIOP_SCORE\":\"Biopsy\",\n",
    "                 \"AnonymizedPatientID\":\"Patient_ID\",\n",
    "                 \"AnonymizedAccessionNumber\":\"Accession_Number\"}\n",
    "                   \n",
    "    datamart_joined_df = datamart_joined_df[col_to_keep]\n",
    "    datamart_joined_df.rename(columns = new_names, inplace=True)\n",
    "    datamart_joined_df.sort_values(by=['Patient_ID','Accession_Number'], inplace=True)\n",
    "\n",
    "    filename = 'datamart_anon.csv'\n",
    "    full_path = os.path.join(target_directory, filename)\n",
    "\n",
    "    datamart_joined_df.to_csv(full_path, index = False)\n",
    "    \n",
    "\n",
    "def concat_csv_files(directory_path, target_file, remove_target=False):\n",
    "    \"\"\"\n",
    "    Concatenate CSV files from a directory (without PROC_ prefix) into a target CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing CSV files.\n",
    "        target_file (str): Path to the target CSV file where merged data will be saved.\n",
    "        remove_target (bool, optional): If True, the target file will be removed before appending. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove the target file if it exists and remove_target is True\n",
    "    if remove_target and os.path.exists(target_file):\n",
    "        os.remove(target_file)\n",
    "\n",
    "    # Get a list of CSV files without the PROC_ prefix\n",
    "    csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv') and not file.startswith('PROC_')]\n",
    "\n",
    "    # Initialize a set to store the appended rows to avoid duplicates\n",
    "    appended_rows = set()\n",
    "\n",
    "    # If the target file exists, read its contents and populate the set\n",
    "    if os.path.exists(target_file):\n",
    "        with open(target_file, 'r', newline='') as target:\n",
    "            target_reader = csv.reader(target)\n",
    "            appended_rows.update(tuple(row) for row in target_reader)\n",
    "\n",
    "    # Loop over each CSV file and append its unique contents to the target file\n",
    "    with open(target_file, 'a', newline='') as target:\n",
    "        target_writer = csv.writer(target)\n",
    "\n",
    "        for index, file in enumerate(csv_files):\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            with open(file_path, 'r', newline='') as source:\n",
    "                source_reader = csv.reader(source)\n",
    "\n",
    "                # Skip the first row (headers) of all files except the first file\n",
    "                if index > 0:\n",
    "                    next(source_reader)\n",
    "\n",
    "                # Append the unique contents of the current CSV file to the target file\n",
    "                for row in source_reader:\n",
    "                    row_tuple = tuple(row)\n",
    "                    if row_tuple not in appended_rows:\n",
    "                        target_writer.writerow(row)\n",
    "                        appended_rows.add(row_tuple)\n",
    "\n",
    "            # Add a PROC_ prefix to the current CSV file after it's appended\n",
    "            os.rename(file_path, os.path.join(directory_path, 'PROC_' + file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867a641-8606-4b5c-892e-13f520cc5c1c",
   "metadata": {},
   "source": [
    "## Total Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3eedb2-239b-4215-90ea-0476708bc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_anon_maps = r'./data_orig/notion_anon_maps/'\n",
    "path_datamart_splits = r'./data_orig/datamart_splits/processed/'\n",
    "path_notion_queries = r'./data_orig/notion_queries/'\n",
    "path_dicom_zips = r'./data_orig/zip/'\n",
    "path_datamart_master = r'./data_orig/'\n",
    "path_anon_map_master = r'./data_orig/'\n",
    "path_data_anon = r'./data_anon/'\n",
    "\n",
    "datamart_master_file = 'master_datamart.csv'\n",
    "anon_map_master_file = 'master_anon_map.csv'\n",
    "\n",
    "full_path_datamart_master_file = os.path.join(path_datamart_master, datamart_master_file)\n",
    "full_path_anon_map_master_file = os.path.join(path_anon_map_master, anon_map_master_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7eff32-0279-4920-9d33-90884d7a7ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR UI: 'anon'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR TM: 'anon'.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_orig/master_anon_map.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR UI: 'anon'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR TM: 'anon'.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR DA: '0101'.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_orig/master_anon_map.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR UI: 'anon'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR TM: 'anon'.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_orig/master_anon_map.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR UI: 'anon'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR TM: 'anon'.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_orig/master_anon_map.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR UI: 'anon'. Please see <https://dicom.nema.org/medical/dicom/current/output/html/part05.html#table_6.2-1> for allowed values for each VR.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pydicom/valuerep.py:443: UserWarning: Invalid value for VR TM: 'anon'.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_orig/master_anon_map.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_orig/zip/00127_dicoms.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m dicoms_zip_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dicoms.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m target_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_data_anon, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dicoms_anon/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mextract_deidentify_dcm_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_dicom_zips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdicoms_zip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# merge anon_map into datamart split file and clean out all PHI\u001b[39;00m\n\u001b[1;32m     23\u001b[0m datamart_csv_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_datamart_splits,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_datamart.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 169\u001b[0m, in \u001b[0;36mextract_deidentify_dcm_file\u001b[0;34m(zip_path, zip_file, target_directory)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Open the ZIP file\u001b[39;00m\n\u001b[1;32m    168\u001b[0m full_path_zip_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(zip_path, zip_file)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path_zip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Loop over each file in the ZIP file\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m zip_ref\u001b[38;5;241m.\u001b[39mnamelist():\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m member\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.dcm\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m             \u001b[38;5;66;03m# Read the DICOM file using PyDICOM\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_orig/zip/00127_dicoms.zip'"
     ]
    }
   ],
   "source": [
    "clean_folders = True\n",
    "if clean_folders:\n",
    "    remove_proc_prefix( path_anon_maps )\n",
    "\n",
    "directory_path = path_anon_maps\n",
    "target_file = full_path_anon_map_master_file\n",
    "concat_csv_files(directory_path, target_file, remove_target=True)\n",
    "\n",
    "# get list of unprocessed datamart_split files\n",
    "datamart_splits_files = [filename for filename in os.listdir(path_datamart_splits) if filename.endswith('.csv') and not filename.startswith('PROC')]\n",
    "\n",
    "# main processing loop\n",
    "for datamart_file in datamart_splits_files:\n",
    "    # get batch number\n",
    "    batch_number_str = datamart_file.split('_')[0]\n",
    "\n",
    "    # process the zip files, deidentify the dicoms, and add them to target directory    \n",
    "    dicoms_zip_file = f'{batch_number_str}_dicoms.zip'\n",
    "    target_directory = os.path.join(path_data_anon, f'{batch_number_str}_dicoms_anon/')\n",
    "    extract_deidentify_dcm_file(path_dicom_zips, dicoms_zip_file, target_directory)\n",
    "\n",
    "    # merge anon_map into datamart split file and clean out all PHI\n",
    "    datamart_csv_file = os.path.join(path_datamart_splits,f'{batch_number_str}_datamart.csv')\n",
    "    merge_anon_into_datamart(datamart_csv_file, full_path_anon_map_master_file, target_directory)\n",
    "    \n",
    "    # add datamart split file to master datamart file and add PROC_ prefix\n",
    "    append_to_csv( full_path_datamart_master_file, datamart_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7e45aca-8913-4c55-8f69-83b522c85644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_orig/master_anon_map.csv\n"
     ]
    }
   ],
   "source": [
    "# merge anon_map into datamart split file and clean out all PHI\n",
    "datamart_csv_file = os.path.join(path_datamart_splits,f'{batch_number_str}_datamart.csv')\n",
    "merge_anon_into_datamart(datamart_csv_file, full_path_anon_map_master_file, target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed43e33b-5078-4472-99f1-0a20fe367cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_csv( full_path_datamart_master_file, datamart_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac11777-fb36-44d5-aa89-15b89e68fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00095_datamart.csv', '00096_datamart.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamart_splits_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c5b572c-ae1b-45ba-9390-d1ac3ebc210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 8147 dicom files found.\n"
     ]
    }
   ],
   "source": [
    "def get_dcm_files(directory):\n",
    "    dcm_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.dcm'):\n",
    "                dcm_files.append(os.path.join(root, file))\n",
    "    \n",
    "    return dcm_files\n",
    "\n",
    "dcm_files = get_dcm_files('./data_anon')\n",
    "\n",
    "print(f'There were {len(dcm_files)} dicom files found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70ca11-4dbd-46cb-b4d0-1cc27bdf5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was one-time code for converting all the datamart split files\n",
    "# to notion query files, keep the code in case notion rejects the converted files\n",
    "\n",
    "def datamart_to_notion_query( datamart_file, notion_query_file):\n",
    "    \"\"\"read datamart csv and write out notion_query xlsx file\n",
    "\n",
    "    Args:\n",
    "        datamart_file:  string with full path filename to datamart file (csv)\n",
    "        notion_query_file:  target filename (xlsx)\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "\n",
    "    \"\"\"\n",
    "    datamart_df = pd.read_csv( datamart_file )\n",
    "\n",
    "    col_names = [\n",
    "        'PatientName', 'PatientID', 'AccessionNumber', \n",
    "        'PatientBirthDate', 'StudyDate', 'ModalitiesInStudy', \n",
    "        'StudyDescription', 'AnonymizedName', 'AnonymizedID']\n",
    "    \n",
    "    notion_query_df = pd.DataFrame(\n",
    "        {'PatientID': datamart_df['PATIENTID'],\n",
    "\t     'AccessionNumber': datamart_df['ACCESSIONNUMBER']}, \n",
    "        columns=col_names)\n",
    "\n",
    "    notion_query_df.to_excel(notion_query_file, index=False)\n",
    "\n",
    "for batch in np.arange(19,127):\n",
    "    batch_string = f'{batch:05}'\n",
    "    datamart_file = f'./data_orig/datamart_splits/unprocessed/{batch_string}_datamart.csv'\n",
    "    notion_query_file = f'./data_orig/notion_queries/{batch_string}_notion_query.xlsx'\n",
    "    datamart_to_notion_query( datamart_file, notion_query_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb077cf3-abcc-478c-b153-068597f6e80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000000'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11918735-641f-4e61-ad9b-4a3b11f932a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def append_to_csv(target_file, input_file, columns):\n",
    "    # Add prefix \"PROC_\" to the input filename\n",
    "    input_dir = os.path.dirname(input_file)\n",
    "    input_filename = os.path.basename(input_file)\n",
    "    input_filename_with_prefix = \"PROC_\" + input_filename\n",
    "    input_file_with_prefix = os.path.join(input_dir, input_filename_with_prefix)\n",
    "\n",
    "    # Check if the target file exists\n",
    "    target_exists = os.path.exists(target_file)\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(input_file, 'r', newline='') as input_csv_file:\n",
    "        input_csv_reader = csv.reader(input_csv_file)\n",
    "        input_rows = list(input_csv_reader)\n",
    "\n",
    "    # Check if the input file has any rows\n",
    "    if len(input_rows) == 0:\n",
    "        print(\"Input file is empty. No rows to append.\")\n",
    "        return\n",
    "\n",
    "    # Determine if the target file already has a header\n",
    "    target_has_header = False\n",
    "    if target_exists:\n",
    "        with open(target_file, 'r', newline='') as target_csv_file:\n",
    "            target_csv_reader = csv.reader(target_csv_file)\n",
    "            target_has_header = next(target_csv_reader, None) is not None\n",
    "\n",
    "    # Open the target file for appending\n",
    "    with open(target_file, 'a', newline='') as target_csv_file:\n",
    "        target_csv_writer = csv.writer(target_csv_file)\n",
    "\n",
    "        # If target file doesn't have a header, write the header from the input file\n",
    "        if not target_has_header:\n",
    "            target_csv_writer.writerow(input_rows[0])\n",
    "\n",
    "        # Write the rows from the input file, skipping duplicate rows and the header if it exists\n",
    "        if target_has_header:\n",
    "            input_rows = input_rows[1:]\n",
    "\n",
    "        # Get the existing rows in the target file\n",
    "        existing_rows = []\n",
    "        if target_exists:\n",
    "            with open(target_file, 'r', newline='') as target_csv_file:\n",
    "                existing_csv_reader = csv.reader(target_csv_file)\n",
    "                existing_rows = list(existing_csv_reader)\n",
    "\n",
    "        # Append only the non-duplicate rows\n",
    "        for row in input_rows:\n",
    "            match_found = False\n",
    "            for existing_row in existing_rows:\n",
    "                if all(existing_row[col] == row[col] for col in columns):\n",
    "                    match_found = True\n",
    "                    break\n",
    "\n",
    "            if not match_found:\n",
    "                target_csv_writer.writerow(row)\n",
    "                existing_rows.append(row)\n",
    "\n",
    "    print(\"Rows appended successfully!\")\n",
    "\n",
    "    # Rename the input file with the \"PROC_\" prefix\n",
    "    os.rename(input_file, input_file_with_prefix)\n",
    "    print(\"Input file renamed with prefix:\", input_file_with_prefix)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "target_csv_file = 'target.csv'\n",
    "input_csv_file = 'input.csv'\n",
    "columns_to_check = [0, 1]  # Example columns to check (0-based indices)\n",
    "\n",
    "append_to_csv(target_csv_file, input_csv_file, columns_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d93f53e-6f85-48b8-aaf7-1375c69f9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_problem1 = pydicom.dcmread('./problem.dcm')\n",
    "ds_problem2 = pydicom.dcmread('./problem2.dcm')\n",
    "ds_no_problem = pydicom.dcmread('./no_problem.dcm')\n",
    "ds_problem3 = pydicom.dcmread('./problem3.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56725cf9-1768-4c2b-bd49-e292165b755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    media_type = ds.file_meta[0x00020002]\n",
    "    is_video = str(media_type).find('Multi-frame')>-1\n",
    "    is_secondary = str(media_type).find('Secondary')>-1\n",
    "    if is_secondary:\n",
    "        y0 = 101\n",
    "    else:\n",
    "        y0 = ds['SequenceOfUltrasoundRegions'][0]['RegionLocationMinY0'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "beb1bc44-a702-406e-851a-ea833798166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset.file_meta -------------------------------\n",
       "(0002, 0000) File Meta Information Group Length  UL: 194\n",
       "(0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\n",
       "(0002, 0002) Media Storage SOP Class UID         UI: Secondary Capture Image Storage\n",
       "(0002, 0003) Media Storage SOP Instance UID      UI: 1.2.840.113713.17.50545756.3788426393483279394079865510187416286\n",
       "(0002, 0010) Transfer Syntax UID                 UI: Explicit VR Little Endian\n",
       "(0002, 0012) Implementation Class UID            UI: 1.2.40.0.13.1.1\n",
       "(0002, 0013) Implementation Version Name         SH: 'dcm4che-1.4.28'\n",
       "-------------------------------------------------\n",
       "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'SMALL PARTS', '0011']\n",
       "(0008, 0012) Instance Creation Date              DA: '20220916'\n",
       "(0008, 0013) Instance Creation Time              TM: '111644'\n",
       "(0008, 0016) SOP Class UID                       UI: Secondary Capture Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.2.840.113713.17.50545756.3788426393483279394079865510187416286\n",
       "(0008, 0020) Study Date                          DA: '20220916'\n",
       "(0008, 0021) Series Date                         DA: '20220916'\n",
       "(0008, 0022) Acquisition Date                    DA: '20220916'\n",
       "(0008, 0023) Content Date                        DA: '20220916'\n",
       "(0008, 002a) Acquisition DateTime                DT: '20220916111644000000'\n",
       "(0008, 0030) Study Time                          TM: '111640'\n",
       "(0008, 0031) Series Time                         TM: '111644'\n",
       "(0008, 0032) Acquisition Time                    TM: '111644'\n",
       "(0008, 0033) Content Time                        TM: '111644'\n",
       "(0008, 0050) Accession Number                    SH: '4781'\n",
       "(0008, 0060) Modality                            CS: 'US'\n",
       "(0008, 0064) Conversion Type                     CS: 'WSD'\n",
       "(0008, 0070) Manufacturer                        LO: 'Imorgon Medical, LLC.'\n",
       "(0008, 0090) Referring Physician's Name          PN: ''\n",
       "(0008, 1030) Study Description                   LO: 'BI ULTRASOUND BREAST FOCUSED LEFT'\n",
       "(0008, 103e) Series Description                  LO: 'BI ULTRASOUND BREAST FOCUSED LEFT'\n",
       "(0010, 0010) Patient's Name                      PN: 'PN-4558'\n",
       "(0010, 0020) Patient ID                          LO: '4522'\n",
       "(0010, 0021) Issuer of Patient ID                LO: 'Notion-50545756'\n",
       "(0010, 0030) Patient's Birth Date                DA: '19500928'\n",
       "(0010, 0032) Patient's Birth Time                TM: '000000'\n",
       "(0010, 0040) Patient's Sex                       CS: 'F'\n",
       "(0010, 1010) Patient's Age                       AS: '071Y'\n",
       "(0010, 1020) Patient's Size                      DS: '1.65'\n",
       "(0010, 1030) Patient's Weight                    DS: '52.3'\n",
       "(0018, 1010) Secondary Capture Device ID         LO: 'Imorgon Report Accelerator'\n",
       "(0018, 1012) Date of Secondary Capture           DA: '20171030'\n",
       "(0018, 1014) Time of Secondary Capture           TM: '111644'\n",
       "(0018, 1016) Secondary Capture Device Manufactur LO: 'Imorgon Medical, LLC.'\n",
       "(0018, 1018) Secondary Capture Device Manufactur LO: 'Imorgon Report Accelerator'\n",
       "(0018, 1019) Secondary Capture Device Software V LO: '1.5'\n",
       "(0018, 9525) Application Version                 LO: '1.5'\n",
       "(0020, 000d) Study Instance UID                  UI: 1.2.840.113713.17.50545756.5548677078555517575904721616472689730\n",
       "(0020, 000e) Series Instance UID                 UI: 1.2.840.113713.17.50545756.4211465578773244593197784396727105840\n",
       "(0020, 0010) Study ID                            SH: ''\n",
       "(0020, 0011) Series Number                       IS: '5717'\n",
       "(0020, 0013) Instance Number                     IS: '1'\n",
       "(0020, 0020) Patient Orientation                 CS: ''\n",
       "(0028, 0002) Samples per Pixel                   US: 3\n",
       "(0028, 0004) Photometric Interpretation          CS: 'RGB'\n",
       "(0028, 0006) Planar Configuration                US: 0\n",
       "(0028, 0010) Rows                                US: 1024\n",
       "(0028, 0011) Columns                             US: 768\n",
       "(0028, 0014) Ultrasound Color Data Present       US: 1\n",
       "(0028, 0100) Bits Allocated                      US: 8\n",
       "(0028, 0101) Bits Stored                         US: 8\n",
       "(0028, 0102) High Bit                            US: 7\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 2110) Lossy Image Compression             CS: '01'\n",
       "(0028, 2112) Lossy Image Compression Ratio       DS: '18.0'\n",
       "(0029, 00e0) Private Creator                     LO: 'TeraMEDICA 7.3.2 B9'\n",
       "(0029, 00e1) Private Creator                     LO: 'FujiFILM TM'\n",
       "(0040, 0260)  Performed Protocol Code Sequence  1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: '00010'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'LOCAL'\n",
       "   (0008, 0104) Code Meaning                        LO: 'ULTRASOUND GENERIC'\n",
       "   ---------\n",
       "(7fe0, 0010) Pixel Data                          OB: Array of 2359296 elements"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_problem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6158801b-8a47-43e6-abe0-54509a7ff411",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = pydicom.dcmread('./no_problem.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8f1c0f7-c86c-4547-8ec1-aad1a9a01b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset.file_meta -------------------------------\n",
       "(0002, 0000) File Meta Information Group Length  UL: 194\n",
       "(0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\n",
       "(0002, 0002) Media Storage SOP Class UID         UI: Secondary Capture Image Storage\n",
       "(0002, 0003) Media Storage SOP Instance UID      UI: 1.2.840.113713.17.50545756.5031917645990166336542510695148645574\n",
       "(0002, 0010) Transfer Syntax UID                 UI: Explicit VR Little Endian\n",
       "(0002, 0012) Implementation Class UID            UI: 1.2.40.0.13.1.1\n",
       "(0002, 0013) Implementation Version Name         SH: 'dcm4che-1.4.28'\n",
       "-------------------------------------------------\n",
       "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'SMALL PARTS', '0001']\n",
       "(0008, 0016) SOP Class UID                       UI: Secondary Capture Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.2.840.113713.17.50545756.5031917645990166336542510695148645574\n",
       "(0008, 0020) Study Date                          DA: '20190218'\n",
       "(0008, 0021) Series Date                         DA: '20190218'\n",
       "(0008, 0023) Content Date                        DA: '20190218'\n",
       "(0008, 0030) Study Time                          TM: '085312'\n",
       "(0008, 0031) Series Time                         TM: '085312'\n",
       "(0008, 0033) Content Time                        TM: '085320'\n",
       "(0008, 0050) Accession Number                    SH: '4712'\n",
       "(0008, 0060) Modality                            CS: 'US'\n",
       "(0008, 0064) Conversion Type                     CS: 'WSD'\n",
       "(0008, 0070) Manufacturer                        LO: 'GE Healthcare'\n",
       "(0008, 0090) Referring Physician's Name          PN: ''\n",
       "(0008, 1030) Study Description                   LO: 'BI ULTRASOUND BREAST FOCUSED LEFT'\n",
       "(0010, 0010) Patient's Name                      PN: 'PN-4493'\n",
       "(0010, 0020) Patient ID                          LO: '4457'\n",
       "(0010, 0021) Issuer of Patient ID                LO: 'Notion-50545756'\n",
       "(0010, 0030) Patient's Birth Date                DA: '19620209'\n",
       "(0010, 0032) Patient's Birth Time                TM: '000000'\n",
       "(0010, 0040) Patient's Sex                       CS: 'F'\n",
       "(0010, 1020) Patient's Size                      DS: '1.64'\n",
       "(0010, 1030) Patient's Weight                    DS: '74.3'\n",
       "(0018, 1010) Secondary Capture Device ID         LO: 'LOGIQE9'\n",
       "(0018, 1012) Date of Secondary Capture           DA: '20131215'\n",
       "(0018, 1014) Time of Secondary Capture           TM: '085320'\n",
       "(0018, 1016) Secondary Capture Device Manufactur LO: 'GE Healthcare'\n",
       "(0018, 1018) Secondary Capture Device Manufactur LO: 'LOGIQE9'\n",
       "(0018, 1019) Secondary Capture Device Software V LO: 'LOGIQE9:R3.1.3'\n",
       "(0020, 000d) Study Instance UID                  UI: 1.2.840.113713.17.50545756.1431196823669136330109917601627634764\n",
       "(0020, 000e) Series Instance UID                 UI: 1.2.840.113713.17.50545756.9483165123451323918207251330179922096\n",
       "(0020, 0010) Study ID                            SH: ''\n",
       "(0020, 0011) Series Number                       IS: '1'\n",
       "(0020, 0013) Instance Number                     IS: '0'\n",
       "(0020, 0020) Patient Orientation                 CS: ''\n",
       "(0028, 0002) Samples per Pixel                   US: 3\n",
       "(0028, 0004) Photometric Interpretation          CS: 'RGB'\n",
       "(0028, 0006) Planar Configuration                US: 0\n",
       "(0028, 0010) Rows                                US: 768\n",
       "(0028, 0011) Columns                             US: 1024\n",
       "(0028, 0100) Bits Allocated                      US: 8\n",
       "(0028, 0101) Bits Stored                         US: 8\n",
       "(0028, 0102) High Bit                            US: 7\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 2110) Lossy Image Compression             CS: '00'\n",
       "(0029, 00e1) Private Creator                     LO: 'FujiFILM TM'\n",
       "(0040, 0260)  Performed Protocol Code Sequence  1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: '43010'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'L'\n",
       "   (0008, 0104) Code Meaning                        LO: 'RAD BI US BREAST FOCUS'\n",
       "   ---------\n",
       "(7fe0, 0010) Pixel Data                          OB: Array of 2359296 elements"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02029a98-f322-40c5-ae0a-9d0c650ea31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0x0018,0x6011) in ds_problem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f654209-9805-43e4-9337-9bde6e37a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_type = ds1.file_meta[0x00020002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19d4a10a-5959-4efe-9682-2bda3d9792e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0002, 0002) Media Storage SOP Class UID         UI: Secondary Capture Image Storage"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53745214-4e9e-4054-9cd2-ea057baa29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89248cc1-ea16-4ef3-b6c0-88c18f3f51ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( media_type == 'image' and (0x0018, 0x6011) in ds) or media_type=='multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "364d29a0-9979-4eb9-8db1-abc163da91c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( media_type == 'image' and (0x0018, 0x6011) in ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfb6f9b8-ece2-471e-ae9c-e067652c3c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_type = ds_problem2.file_meta[0x00020002]\n",
    "media_type=='image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee050c3d-86cd-4c33-9171-f762bd8df0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset.file_meta -------------------------------\n",
       "(0002, 0000) File Meta Information Group Length  UL: 194\n",
       "(0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\n",
       "(0002, 0002) Media Storage SOP Class UID         UI: Ultrasound Multi-frame Image Storage\n",
       "(0002, 0003) Media Storage SOP Instance UID      UI: 1.2.840.113713.17.50545756.7983231200671517409213250989291907106\n",
       "(0002, 0010) Transfer Syntax UID                 UI: Implicit VR Little Endian\n",
       "(0002, 0012) Implementation Class UID            UI: 1.2.40.0.13.1.1\n",
       "(0002, 0013) Implementation Version Name         SH: 'dcm4che-1.4.28'\n",
       "-------------------------------------------------\n",
       "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', '', '0001']\n",
       "(0008, 0016) SOP Class UID                       UI: Ultrasound Multi-frame Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.2.840.113713.17.50545756.7983231200671517409213250989291907106\n",
       "(0008, 0020) Study Date                          DA: '20190218'\n",
       "(0008, 0021) Series Date                         DA: '20190218'\n",
       "(0008, 0023) Content Date                        DA: '20190218'\n",
       "(0008, 0030) Study Time                          TM: '085312'\n",
       "(0008, 0031) Series Time                         TM: '085312'\n",
       "(0008, 0033) Content Time                        TM: '085749'\n",
       "(0008, 0050) Accession Number                    SH: '4712'\n",
       "(0008, 0060) Modality                            CS: 'US'\n",
       "(0008, 0070) Manufacturer                        LO: 'GE Healthcare'\n",
       "(0008, 0090) Referring Physician's Name          PN: ''\n",
       "(0008, 1030) Study Description                   LO: 'BI ULTRASOUND BREAST FOCUSED LEFT'\n",
       "(0008, 2142) Start Trim                          IS: '1'\n",
       "(0008, 2143) Stop Trim                           IS: '81'\n",
       "(0008, 2144) Recommended Display Frame Rate      IS: '19'\n",
       "(0010, 0010) Patient's Name                      PN: 'PN-4493'\n",
       "(0010, 0020) Patient ID                          LO: '4457'\n",
       "(0010, 0021) Issuer of Patient ID                LO: 'Notion-50545756'\n",
       "(0010, 0030) Patient's Birth Date                DA: '19620209'\n",
       "(0010, 0032) Patient's Birth Time                TM: '000000'\n",
       "(0010, 0040) Patient's Sex                       CS: 'F'\n",
       "(0010, 1020) Patient's Size                      DS: '1.64'\n",
       "(0010, 1030) Patient's Weight                    DS: '74.3'\n",
       "(0018, 0040) Cine Rate                           IS: '19'\n",
       "(0018, 0072) Effective Duration                  DS: '4.263158'\n",
       "(0018, 1063) Frame Time                          DS: '52.45696'\n",
       "(0018, 1066) Frame Delay                         DS: '0.0'\n",
       "(0018, 1088) Heart Rate                          IS: '-1'\n",
       "(0018, 1242) Actual Frame Duration               IS: '52'\n",
       "(0018, 1244) Preferred Playback Sequencing       US: 0\n",
       "(0020, 000d) Study Instance UID                  UI: 1.2.840.113713.17.50545756.1431196823669136330109917601627634764\n",
       "(0020, 000e) Series Instance UID                 UI: 1.2.840.113713.17.50545756.9483165123451323918207251330179922096\n",
       "(0020, 0010) Study ID                            SH: ''\n",
       "(0020, 0011) Series Number                       IS: '1'\n",
       "(0020, 0013) Instance Number                     IS: '2304'\n",
       "(0020, 0020) Patient Orientation                 CS: ''\n",
       "(0028, 0002) Samples per Pixel                   US: 3\n",
       "(0028, 0004) Photometric Interpretation          CS: 'RGB'\n",
       "(0028, 0006) Planar Configuration                US: 0\n",
       "(0028, 0008) Number of Frames                    IS: '81'\n",
       "(0028, 0009) Frame Increment Pointer             AT: (0018, 1063)\n",
       "(0028, 0010) Rows                                US: 519\n",
       "(0028, 0011) Columns                             US: 680\n",
       "(0028, 0014) Ultrasound Color Data Present       US: 1\n",
       "(0028, 0100) Bits Allocated                      US: 8\n",
       "(0028, 0101) Bits Stored                         US: 8\n",
       "(0028, 0102) High Bit                            US: 7\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 2110) Lossy Image Compression             CS: '00'\n",
       "(0029, 00e1) Private Creator                     VR.LO: 'FujiFILM TM'\n",
       "(0040, 0260)  Performed Protocol Code Sequence  1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: '43010'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'L'\n",
       "   (0008, 0104) Code Meaning                        LO: 'RAD BI US BREAST FOCUS'\n",
       "   ---------\n",
       "(7fe0, 0010) Pixel Data                          VR.OW: Array of 85759560 elements"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_problem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12c73762-d179-4d2a-b015-bbec2683d8bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0018, 6011)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_new \u001b[38;5;241m=\u001b[39m \u001b[43mdeidentify_dicom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_problem3\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 48\u001b[0m, in \u001b[0;36mdeidentify_dicom_dataset\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     46\u001b[0m     y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     y0 \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSequenceOfUltrasoundRegions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegionLocationMinY0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginalAttributesSequence\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ds:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mOriginalAttributesSequence\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pydicom/dataset.py:988\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m--> 988\u001b[0m elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, RawDataElement):\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# If a deferred read, then go get the value now\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: (0018, 6011)"
     ]
    }
   ],
   "source": [
    " ds_new = deidentify_dicom_dataset(ds_problem3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e1b2112-4db0-44c7-b66c-32e9afaf8916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset.file_meta -------------------------------\n",
       "(0002, 0000) File Meta Information Group Length  UL: 194\n",
       "(0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\n",
       "(0002, 0002) Media Storage SOP Class UID         UI: Secondary Capture Image Storage\n",
       "(0002, 0003) Media Storage SOP Instance UID      UI: anon\n",
       "(0002, 0010) Transfer Syntax UID                 UI: Explicit VR Little Endian\n",
       "(0002, 0012) Implementation Class UID            UI: anon\n",
       "(0002, 0013) Implementation Version Name         SH: 'dcm4che-1.4.28'\n",
       "-------------------------------------------------\n",
       "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'SMALL PARTS', '0011']\n",
       "(0008, 0012) Instance Creation Date              DA: '20220101'\n",
       "(0008, 0013) Instance Creation Time              TM: 'anon'\n",
       "(0008, 0016) SOP Class UID                       UI: Secondary Capture Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: anon\n",
       "(0008, 0020) Study Date                          DA: '20220101'\n",
       "(0008, 0021) Series Date                         DA: '20220101'\n",
       "(0008, 0022) Acquisition Date                    DA: '20220101'\n",
       "(0008, 0023) Content Date                        DA: '20220101'\n",
       "(0008, 002a) Acquisition DateTime                DT: '20220916111644000000'\n",
       "(0008, 0030) Study Time                          TM: 'anon'\n",
       "(0008, 0031) Series Time                         TM: 'anon'\n",
       "(0008, 0032) Acquisition Time                    TM: 'anon'\n",
       "(0008, 0033) Content Time                        TM: 'anon'\n",
       "(0008, 0050) Accession Number                    SH: '4781'\n",
       "(0008, 0060) Modality                            CS: 'US'\n",
       "(0008, 0064) Conversion Type                     CS: 'WSD'\n",
       "(0008, 0070) Manufacturer                        LO: 'Imorgon Medical, LLC.'\n",
       "(0008, 0090) Referring Physician's Name          PN: ''\n",
       "(0008, 1030) Study Description                   LO: 'BI ULTRASOUND BREAST FOCUSED LEFT'\n",
       "(0008, 103e) Series Description                  LO: 'BI ULTRASOUND BREAST FOCUSED LEFT'\n",
       "(0010, 0010) Patient's Name                      PN: 'PN-4558'\n",
       "(0010, 0020) Patient ID                          LO: '4522'\n",
       "(0010, 0021) Issuer of Patient ID                LO: 'Notion-50545756'\n",
       "(0010, 0030) Patient's Birth Date                DA: '19500101'\n",
       "(0010, 0032) Patient's Birth Time                TM: 'anon'\n",
       "(0010, 0040) Patient's Sex                       CS: 'F'\n",
       "(0010, 1010) Patient's Age                       AS: '071Y'\n",
       "(0010, 1020) Patient's Size                      DS: '1.65'\n",
       "(0010, 1030) Patient's Weight                    DS: '52.3'\n",
       "(0018, 1010) Secondary Capture Device ID         LO: 'Imorgon Report Accelerator'\n",
       "(0018, 1012) Date of Secondary Capture           DA: '20170101'\n",
       "(0018, 1014) Time of Secondary Capture           TM: 'anon'\n",
       "(0018, 1016) Secondary Capture Device Manufactur LO: 'Imorgon Medical, LLC.'\n",
       "(0018, 1018) Secondary Capture Device Manufactur LO: 'Imorgon Report Accelerator'\n",
       "(0018, 1019) Secondary Capture Device Software V LO: '1.5'\n",
       "(0018, 9525) Application Version                 LO: '1.5'\n",
       "(0020, 000d) Study Instance UID                  UI: anon\n",
       "(0020, 000e) Series Instance UID                 UI: anon\n",
       "(0020, 0010) Study ID                            SH: ''\n",
       "(0020, 0011) Series Number                       IS: '5717'\n",
       "(0020, 0013) Instance Number                     IS: '1'\n",
       "(0020, 0020) Patient Orientation                 CS: ''\n",
       "(0028, 0002) Samples per Pixel                   US: 3\n",
       "(0028, 0004) Photometric Interpretation          CS: 'RGB'\n",
       "(0028, 0006) Planar Configuration                US: 0\n",
       "(0028, 0010) Rows                                US: 1024\n",
       "(0028, 0011) Columns                             US: 768\n",
       "(0028, 0014) Ultrasound Color Data Present       US: 1\n",
       "(0028, 0100) Bits Allocated                      US: 8\n",
       "(0028, 0101) Bits Stored                         US: 8\n",
       "(0028, 0102) High Bit                            US: 7\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 2110) Lossy Image Compression             CS: '01'\n",
       "(0028, 2112) Lossy Image Compression Ratio       DS: '18.0'\n",
       "(0040, 0260)  Performed Protocol Code Sequence  1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: '00010'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'LOCAL'\n",
       "   (0008, 0104) Code Meaning                        LO: 'ULTRASOUND GENERIC'\n",
       "   ---------\n",
       "(7fe0, 0010) Pixel Data                          OB: Array of 2359296 elements"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26e02e-58df-4288-bf30-55ee1fb8bdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
